{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bcf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from requests.exceptions import ConnectionError\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts property details from real estate advertisements.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit method, required by scikit-learn, does nothing in this case.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, html):\n",
    "        \"\"\"Extracts property details without relying on unstable class names.\"\"\"\n",
    "        \n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Find unique advertisement containers (assumed to be divs with at least one property link)\n",
    "        ad_containers = set(ad.find_parent(\"div\") for ad in soup.find_all(\"a\", href=True, target=\"_blank\"))\n",
    "\n",
    "        extracted_data = []\n",
    "\n",
    "        for container in ad_containers:\n",
    "            if container:\n",
    "                # Extract URL - Take only the first valid <a> link within the container\n",
    "                a_tag = container.find(\"a\", href=True, target=\"_blank\")\n",
    "                url = a_tag[\"href\"] if a_tag else None\n",
    "\n",
    "                # Extract Title - Look for the nearest <h2> inside the container\n",
    "                title_elem = container.find(\"h2\")\n",
    "                title = title_elem.text.strip() if title_elem else None\n",
    "\n",
    "                # Extract Price - Find first <p> containing \"€\"\n",
    "                price_elem = container.find(\"p\", string=lambda text: text and (\"€\" in text or \"Cena Dohodou\" in text))\n",
    "                price = price_elem.text.strip() if price_elem else None\n",
    "\n",
    "                # Extract Price per m² - Only extract if Price is NOT \"Cena Dohodou\"\n",
    "                price_per_m_elem = None\n",
    "                price_per_m = None\n",
    "                if price and \"Cena Dohodou\" not in price:\n",
    "                    price_per_m_elem = container.find(\"p\", string=lambda text: text and \"€/m²\" in text)\n",
    "                    price_per_m = price_per_m_elem.text.strip() if price_per_m_elem else None\n",
    "\n",
    "                # Extract Property Size - Look for <p> containing \"m²\"\n",
    "                size_elem = container.find(\"p\", string=lambda text: text and \"m²\" in text)\n",
    "                size = size_elem.text.strip() if size_elem else None\n",
    "\n",
    "                # Extract Address using specific \"data-test-id\" attribute\n",
    "                address_elem = container.find(\"p\", {\"data-test-id\": \"text\"}, string=lambda text: text and \",\" in text)\n",
    "                address = address_elem.text.strip() if address_elem else None\n",
    "\n",
    "                # Extract Property Type - Look for a <p> that does not contain \"m²\" or \"€\"\n",
    "                property_type_elem = container.find(\"p\", {\"data-test-id\": \"text\"}, string=lambda text: text and \"€\" not in text and \"m²\" not in text and \",\" not in text)\n",
    "                property_type = property_type_elem.text.strip() if property_type_elem else None\n",
    "\n",
    "                # Append extracted values if the title and URL are valid\n",
    "                if title and url:\n",
    "                    extracted_data.append({\n",
    "                        \"Title\": title,\n",
    "                        \"Url\": url,\n",
    "                        \"Property_Type\": property_type,\n",
    "                        \"Size\": size,\n",
    "                        \"Price\": price,\n",
    "                        \"Price_per_m²\": price_per_m,\n",
    "                        \"Address\": address\n",
    "                    })\n",
    "        # Convert extracted data into a DataFrame\n",
    "        return pd.DataFrame(extracted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ab362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform method that processes a DataFrame of URLs and other columns.\"\"\"\n",
    "        extracted_data = []\n",
    "        \n",
    "        # Loop over each row in the DataFrame to process each URL and include the other columns\n",
    "        for index, row in X.iterrows():\n",
    "            url = row['Url']\n",
    "            size = row['Size']\n",
    "            property_type = row['Property_Type']\n",
    "            \n",
    "            # Remove 'm²' from the size column if present\n",
    "            size_cleaned = self.clean_size(size)\n",
    "            \n",
    "            # Extract the information from the URL\n",
    "            data = self.extract_info(url, row)\n",
    "            \n",
    "            # Extract Semantic Metadata (texts inside MuiGrid-container until \"MAKLÉR\" is found)\n",
    "            semantic_metadata = self.extract_semantic_metadata(url)\n",
    "            \n",
    "            # Add the size and property_type from X to the extracted data\n",
    "            data['size_of_property'] = size_cleaned\n",
    "            data['type_of_property'] = property_type\n",
    "            data['url'] = url  # Add the URL to the extracted data\n",
    "            data['description_text'] = data['description_text'] + \" \" + str(semantic_metadata)\n",
    "\n",
    "            \n",
    "            extracted_data.append(data)\n",
    "        \n",
    "        # Return the extracted data as a DataFrame\n",
    "        return pd.DataFrame(extracted_data)\n",
    "\n",
    "    def extract_info(self, url, data):\n",
    "        \"\"\"Extract information from a single URL.\"\"\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, allow_redirects=True)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        time.sleep(0.5)  # Be respectful of server rate limits\n",
    "        clean_phone_number = self.extract_phone_number(soup)\n",
    "        image = self.extract_image(soup)\n",
    "        address = data['Address']\n",
    "        title = data['Title']\n",
    "        type_of_property = data[\"Property_Type\"]\n",
    "        size_of_property = self.extract_property_size(soup)\n",
    "        description_text = self.extract_description(soup)\n",
    "        transaction_type = self.extract_transaction_type(soup, data['Title'], description_text)\n",
    "        price, price_per_m, price_per_month, price_per_m_per_month = self.extract_price_info(soup, transaction_type)\n",
    "        \n",
    "        return {\n",
    "            \"title\":title,\n",
    "            \"address\": address,\n",
    "            \"type_of_property\": type_of_property,\n",
    "            \"size_of_property\": size_of_property,\n",
    "            \"price\": price,\n",
    "            \"price_per_m\": price_per_m,\n",
    "            \"price_per_month\": price_per_month,\n",
    "            \"price_per_m_per_month\": price_per_m_per_month,\n",
    "            \"description_text\": description_text,\n",
    "            \"transaction_type\": transaction_type,\n",
    "            \"clean_phone_number\": clean_phone_number,\n",
    "            \"image\": image\n",
    "        }\n",
    "\n",
    "    def extract_semantic_metadata(self, url):\n",
    "        \"\"\"Extract texts from MuiGrid-container with data-test-id='text' until 'MAKLÉR' is found.\"\"\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the container with id=\"main-detail-content\"\n",
    "        main_detail_content = soup.find(id=\"main-detail-content\")\n",
    "        \n",
    "        # If the container is found, search for the MuiGrid-container within it\n",
    "        if main_detail_content:\n",
    "            # Find all elements with class=\"MuiGrid-container\" inside the main-detail-content\n",
    "            mui_grid_containers = main_detail_content.find_all('div', class_='MuiGrid-container')\n",
    "\n",
    "            # Initialize a list to store the extracted texts\n",
    "            texts = []\n",
    "\n",
    "            # Loop through each MuiGrid-container and extract the texts with data-test-id=\"text\"\n",
    "            for container in mui_grid_containers:\n",
    "                elements = container.find_all(attrs={\"data-test-id\": \"text\"})\n",
    "                for element in elements:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    \n",
    "                    # Stop collecting if we encounter the target text \"MAKLÉR\"\n",
    "                    if \"MAKLÉR\" in text:\n",
    "                        return texts  # Return collected texts without including this one\n",
    "                    \n",
    "                    texts.append(text)\n",
    "            \n",
    "            return texts\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def clean_size(self, size):\n",
    "        \"\"\"Ensure size is a string, remove 'm²' if present, and handle NaN.\"\"\"\n",
    "        if pd.isna(size):  # Check if the size is NaN\n",
    "            return size  # Return NaN if the value is NaN\n",
    "\n",
    "        if not isinstance(size, str):  # If size is not a string, convert it\n",
    "            size = str(size)\n",
    "\n",
    "        # Remove 'm²', extra spaces, and any non-numeric characters\n",
    "        size_cleaned = re.sub(r'[^\\d.,]', '', size)  # Keep digits, commas, and periods\n",
    "        return size_cleaned\n",
    "\n",
    "    def extract_phone_number(self, soup):\n",
    "        a_tag = soup.find_all('a', href=lambda href: href and href.startswith('tel:'))\n",
    "        if a_tag:\n",
    "            phone_number = a_tag[1]['href'].replace('tel:', '')\n",
    "            clean_phone_number = phone_number.replace(' ', '').replace('&nbsp;', '')\n",
    "        else:\n",
    "            clean_phone_number = 'None'\n",
    "        return clean_phone_number\n",
    "\n",
    "    def extract_image(self, soup):\n",
    "        img_tag = soup.find('img')\n",
    "        return img_tag['src'] if img_tag else None\n",
    "\n",
    "    def extract_title_and_address(self, soup):\n",
    "        main_detail_content = soup.find('div', id='main-detail-content')\n",
    "        first_box = main_detail_content.find('div', class_='MuiBox-root') if main_detail_content else None\n",
    "        address = self.extract_text(first_box.find('p', class_='MuiTypography-body2')) if first_box else \"Not found\"\n",
    "        return address\n",
    "\n",
    "    def extract_property_type(self, soup):\n",
    "        type_of_property = \"Not found\"\n",
    "        type_label = soup.find('span', string=\"Druh\")\n",
    "        if type_label:\n",
    "            type_value = type_label.find_next('span')\n",
    "            if type_value:\n",
    "                type_of_property = type_value.get_text(strip=True)\n",
    "        return type_of_property\n",
    "\n",
    "    def extract_property_size(self, soup):\n",
    "        size_of_property = \"Not found\"\n",
    "        size_label = soup.find('span', string=\"Úžitková plocha\")\n",
    "        if size_label:\n",
    "            size_value = size_label.find_next('span')\n",
    "            if size_value:\n",
    "                size_of_property = size_value.get_text(strip=True)\n",
    "        return size_of_property\n",
    "\n",
    "    def extract_description(self, soup):\n",
    "        description_text = \"None\"\n",
    "        h3_tag = soup.find('h3', string=\"Popis nehnuteľnosti\")\n",
    "        if h3_tag:\n",
    "            p_tag = h3_tag.find_next('p')\n",
    "            if p_tag:\n",
    "                description_text = p_tag.get_text(strip=True)\n",
    "        return description_text\n",
    "\n",
    "    def extract_transaction_type(self, soup, title, description_text):\n",
    "        transaction_type = \"Not found\"\n",
    "        if \"kúpa\" in title.lower() or \"kúpa \" in description_text.lower() or \"kúpim \" in title.lower() or \"kúpim \" in description_text.lower():\n",
    "            transaction_type = \"kúpa\"\n",
    "        elif \"predaj\" in title.lower() or \"predaj \" in description_text.lower() or \"predám\" in title.lower() or \"predám\" in description_text.lower():\n",
    "            transaction_type = \"predaj\"\n",
    "        elif \"prenájom \" in title.lower() or \"prenájom \" in description_text.lower() or \"prenajmem\" in title.lower() or \"prenajmem\" in description_text.lower():\n",
    "            transaction_type = \"prenájom\"\n",
    "        \n",
    "        # If transaction type is still \"Not found\", check the #main-detail-content container for keywords\n",
    "        if transaction_type == \"Not found\":\n",
    "            main_detail_content = soup.find(id=\"main-detail-content\")\n",
    "            if main_detail_content:\n",
    "                if \"kúpa\" in main_detail_content.get_text().lower():\n",
    "                    transaction_type = \"kúpa\"\n",
    "                elif \"predaj\" in main_detail_content.get_text().lower():\n",
    "                    transaction_type = \"predaj\"\n",
    "                elif \"prenájom\" in main_detail_content.get_text().lower():\n",
    "                    transaction_type = \"prenájom\"\n",
    "        \n",
    "        return transaction_type\n",
    "\n",
    "    def extract_price_info(self, soup, transaction_type):\n",
    "        price, price_per_m, price_per_month, price_per_m_per_month = None, None, None, None\n",
    "        main_detail_content = soup.find('div', id='main-detail-content')\n",
    "        first_box = main_detail_content.find('div', class_='mui-19idom') if main_detail_content else None\n",
    "        if transaction_type == \"predaj\":\n",
    "            price_raw = self.extract_text(first_box.find('p', class_='MuiTypography-h3')) if first_box else None\n",
    "            if price_raw and \"Cena dohodou\" not in price_raw:\n",
    "                price_cleaned = re.sub(r'[^\\d]', '', price_raw)\n",
    "                price = int(price_cleaned) if price_cleaned else None\n",
    "                price_per_m2_raw = self.extract_text(first_box.find('p', class_='MuiTypography-label2')) if first_box else None\n",
    "                if price_per_m2_raw and \"Cena dohodou\" not in price_per_m2_raw:\n",
    "                    price_per_m2_cleaned = re.sub(r'[^\\d,]', '', price_per_m2_raw).replace(',', '.')\n",
    "                    try:\n",
    "                        price_per_m = float(price_per_m2_cleaned) if price_per_m2_cleaned else None\n",
    "                    except ValueError:\n",
    "                        price_per_m = None\n",
    "            \n",
    "        elif transaction_type == \"prenájom\":\n",
    "            price_per_month_raw = self.extract_text(first_box.find('p', class_='MuiTypography-h3')) if first_box else None\n",
    "            if price_per_month_raw and \"Cena dohodou\" not in price_per_month_raw and price_per_month_raw:\n",
    "                price_per_month_cleaned = re.sub(r'[^\\d]', '', price_per_month_raw)\n",
    "                price_per_month = int(price_per_month_cleaned) if price_per_month_cleaned else None\n",
    "\n",
    "                price_per_m_per_month_raw = self.extract_text(first_box.find('p', class_='MuiTypography-label2')) if first_box else None\n",
    "                if price_per_m_per_month_raw and \"Cena dohodou\" not in price_per_m_per_month_raw:\n",
    "                    price_per_m_per_month_cleaned = re.sub(r'[^\\d,]', '', price_per_m_per_month_raw).replace(',', '.')\n",
    "                    try:\n",
    "                        price_per_m_per_month = float(price_per_m_per_month_cleaned) if price_per_m_per_month_cleaned else None\n",
    "                    except ValueError:\n",
    "                        price_per_m_per_month = None\n",
    "        \n",
    "        return price, price_per_m, price_per_month, price_per_m_per_month\n",
    "\n",
    "    def extract_text(self, tag):\n",
    "        return tag.get_text(strip=True) if tag else \"Not found\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bc17a",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb51f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "nehnutelnosti_sk_pipeline = Pipeline(steps=[\n",
    "    ('value_extractor', ValueExtractor()),\n",
    "    ('info_extractor', InfoExtractor())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a64b7",
   "metadata": {},
   "source": [
    "# Startup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c196bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define category URLs\n",
    "categories = {\n",
    "    \"byty_predaj\": \"https://www.nehnutelnosti.sk/vysledky/byty/predaj?page=\",\n",
    "    \"domy_predaj\": \"https://www.nehnutelnosti.sk/vysledky/domy/predaj?page=\",\n",
    "    \"pozemky_predaj\": \"https://www.nehnutelnosti.sk/vysledky/pozemky/predaj?page=\",\n",
    "    \"rekreacne_predaj\": \"https://www.nehnutelnosti.sk/vysledky/rekreacne-nehnutelnosti/predaj?page=\",\n",
    "    \"priestory_predaj\": \"https://www.nehnutelnosti.sk/vysledky/priestory-a-objekty/predaj?page=\",\n",
    "    \"prenajom\": \"https://www.nehnutelnosti.sk/vysledky/prenajom?page=\",\n",
    "    \"kupa\": \"https://www.nehnutelnosti.sk/vysledky/kupa?page=\"\n",
    "}\n",
    "\n",
    "# Headers to simulate a real browser request\n",
    "headers = {\n",
    "    'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# Assuming nehnutelnosti_sk_pipeline is defined elsewhere\n",
    "new_pipeline = Pipeline(steps=[\n",
    "    ('real_estate_pipeline', nehnutelnosti_sk_pipeline)\n",
    "])\n",
    "\n",
    "def fetch_page_html(base_url, page_number):\n",
    "    url = f\"{base_url}{page_number}\"\n",
    "    response = requests.get(url, headers=headers, allow_redirects=False)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scrape_with_retries(base_url, start_page, end_page, max_retries=20, delay=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return scrape_multiple_pages(base_url, start_page, end_page)\n",
    "        except ConnectionError as e:\n",
    "            print(f\"Connection error: {e}. Retrying {retries + 1}/{max_retries}...\")\n",
    "            retries += 1\n",
    "            time.sleep(delay)\n",
    "    print(f\"Failed after {max_retries} attempts.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def scrape_multiple_pages(base_url, start_page, end_page):\n",
    "    all_data = pd.DataFrame()\n",
    "    seen_urls = set()\n",
    "\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        print(f\"Scraping page {page}\")\n",
    "        raw_html = fetch_page_html(base_url, page)\n",
    "        if raw_html:\n",
    "            result_df = new_pipeline.fit_transform(raw_html)\n",
    "            if isinstance(result_df, pd.DataFrame):\n",
    "                if 'url' in result_df.columns:\n",
    "                    result_df = result_df[~result_df['url'].isin(seen_urls)]\n",
    "                    seen_urls.update(result_df['url'].dropna().tolist())\n",
    "                else:\n",
    "                    print(\"Warning: 'url' column not found; deduplication skipped.\")\n",
    "                all_data = pd.concat([all_data, result_df], ignore_index=True)\n",
    "            else:\n",
    "                print(f\"Pipeline did not return DataFrame on page {page}. Skipping.\")\n",
    "    all_data.insert(0, 'id', range(1, len(all_data) + 1))\n",
    "    return all_data\n",
    "\n",
    "# Scrape each category\n",
    "for category, base_url in categories.items():\n",
    "    total_pages = 40  \n",
    "    pages_per_file = 1\n",
    "    for start_page in range(1, total_pages + 1, pages_per_file):\n",
    "        end_page = min(start_page + pages_per_file - 1, total_pages)\n",
    "        result_df = scrape_with_retries(base_url, start_page, end_page)\n",
    "        if result_df.empty:\n",
    "            print(f\"Skipping {category} pages {start_page}-{end_page} due to repeated failures.\")\n",
    "            continue\n",
    "        file_name = f'{category}_Pack{(start_page - 1) // pages_per_file + 1}.json'\n",
    "        result_json = result_df.to_json(orient=\"records\", lines=False, force_ascii=False)\n",
    "        formatted_json = json.dumps(json.loads(result_json), indent=4, ensure_ascii=False)\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(formatted_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0656126",
   "metadata": {},
   "source": [
    "# Merge Listings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically find all per-category JSON files (like byty_predaj_Pack1.json, etc.)\n",
    "file_names = sorted(glob.glob(\"*_Pack*.json\"))\n",
    "\n",
    "merged_data = []\n",
    "seen_urls = set()\n",
    "current_id = 1\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        for entry in data:\n",
    "            url = entry.get(\"url\")\n",
    "            if url and url not in seen_urls:\n",
    "                entry[\"id\"] = current_id\n",
    "                current_id += 1\n",
    "                seen_urls.add(url)\n",
    "                merged_data.append(entry)\n",
    "\n",
    "# Save to one merged JSON file\n",
    "with open(\"Merged_Listings.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(merged_data, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Merged {len(file_names)} files with {len(merged_data)} unique listings into 'Merged_Listings.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f92be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
